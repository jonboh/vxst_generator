{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('../financial_utils/')\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import performance as per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return_len = 2\n",
    "return_shift = 1\n",
    "vix_len = 10\n",
    "vix_shift = 10\n",
    "vxst_len = vix_len-1\n",
    "vxst_shift = vxst_len+1\n",
    "\n",
    "h1_units = (return_len+vix_len+vxst_len)*4\n",
    "h2_units = h1_units\n",
    "#h3_units = 100\n",
    "step = 1e-6\n",
    "steps = 100000\n",
    "batch_size = 64\n",
    "refresh_rate = 1000\n",
    "load_model = True\n",
    "\n",
    "model_dir = 'models'\n",
    "model_file = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu64():\n",
    "    # By default TF uses tf.float32 in alpha. If tf.nn.leaky_relu is used\n",
    "    # directly in activation of layer.dense, it'll raise an error.\n",
    "    a=tf.constant(0.2, name='alpha', dtype=tf.float64)\n",
    "    return lambda features: tf.nn.leaky_relu(features, alpha=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/first\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model_graph = tf.Graph()\n",
    "sess = tf.Session(graph=model_graph)\n",
    "with model_graph.as_default():\n",
    "    # NET SPECS\n",
    "    with tf.variable_scope('NET'):\n",
    "        x = tf.placeholder(name='x', shape=(None, return_len+vix_len+vxst_len), dtype=tf.float64)\n",
    "        y = tf.placeholder(name='y', shape=(None, 1), dtype=tf.float64)\n",
    "        H1 = tf.layers.dense(name = 'H1',inputs=x, units = h1_units, activation=tf.nn.tanh)\n",
    "        H2 = tf.layers.dense(name = 'H2',inputs=H1, units = h2_units, activation=tf.nn.tanh)\n",
    "        #H3 = tf.layers.dense(name = 'H3',inputs=H2, units = h3_units, activation=tf.nn.tanh)\n",
    "        output = tf.layers.dense(name = 'output', inputs=H1, units=1, activation=tf.nn.tanh)\n",
    "    # TRAIN ALGO SPECS\n",
    "    loss = tf.losses.mean_squared_error(y, output)\n",
    "    train_step = tf.train.AdamOptimizer(step).minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "if not isdir(model_dir):\n",
    "    mkdir(model_dir)\n",
    "    \n",
    "# INITIALIZATION\n",
    "if load_model:\n",
    "    saver.restore(sess, model_dir+'/'+model_file)\n",
    "else:\n",
    "    sess.run(init)\n",
    "    saver.save(sess, model_dir+'/'+model_file)\n",
    "    \n",
    "writer = tf.summary.FileWriter(model_dir+'/graph', sess.graph)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dir = 'history_files'\n",
    "price_file = 'SPX.csv'\n",
    "vix_file = 'VIX.csv'\n",
    "vxst_file = 'VXST.csv'\n",
    "\n",
    "price_table = pd.read_csv(history_dir+'/'+price_file, parse_dates=[0])\n",
    "vix_table = pd.read_csv(history_dir+'/'+vix_file, parse_dates=[0])\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "vxst_table = pd.read_csv(history_dir+'/'+vxst_file, parse_dates=[0], date_parser=dateparse)\n",
    "\n",
    "price_table.drop(['Open', 'Low','High', 'AdjOpen', 'AdjHigh', 'AdjLow','AdjClose'], axis=1, inplace=True)\n",
    "price_table.columns = ['Date', 'Tick']\n",
    "returns = per.tick2ret(price_table)\n",
    "return_table = pd.DataFrame.from_dict({'Date':price_table.Date[1:], 'Return': returns[:,0]})\n",
    "\n",
    "vix_table.drop(['Open', 'Low','High', 'AdjOpen', 'AdjHigh', 'AdjLow','AdjClose'], axis=1, inplace=True)\n",
    "vix_table.Close = vix_table.Close/100\n",
    "vxst_table.drop(['Open', 'Low','High'], axis=1, inplace=True)\n",
    "vxst_table.Close = vxst_table.Close/100\n",
    "\n",
    "dataset = pd.merge(pd.merge(return_table, vix_table, how='inner', on=['Date', 'Date']),vxst_table, how='inner', on=['Date','Date'])\n",
    "dataset.columns = ['Date','Return', 'VIX', 'VXST']\n",
    "dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD NORM\n",
    "dataset_norm = copy.deepcopy(dataset)\n",
    "\n",
    "return_mean = np.mean(dataset_norm.Return)\n",
    "return_std = np.std(dataset_norm.Return)\n",
    "dataset_norm.Return = (dataset_norm.Return - return_mean) / return_std\n",
    "return_maxnorm = np.max(dataset_norm.Return)\n",
    "dataset_norm.Return = dataset_norm.Return / return_maxnorm\n",
    "\n",
    "vix_mean = np.mean(dataset_norm.VIX)\n",
    "vix_std = np.std(dataset_norm.VIX)\n",
    "dataset_norm.VIX = (dataset_norm.VIX - vix_mean) / vix_std\n",
    "vix_maxnorm = np.max(dataset_norm.VIX)\n",
    "dataset_norm.VIX = dataset_norm.VIX / vix_maxnorm\n",
    "\n",
    "vxst_mean = np.mean(dataset_norm.VXST)\n",
    "vxst_std = np.std(dataset_norm.VXST)\n",
    "dataset_norm.VXST = (dataset_norm.VXST - vxst_mean) / vxst_std\n",
    "vxst_maxnorm = np.max(dataset_norm.VXST)\n",
    "dataset_norm.VXST = dataset_norm.VXST / vxst_maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size):\n",
    "    x = np.nan * np.ones((batch_size, return_len+vix_len+vxst_len))\n",
    "    y = np.nan * np.ones((batch_size, 1))\n",
    "    while True:\n",
    "        for j in range(0, batch_size):\n",
    "            i = np.random.randint(low=np.max([return_len,vix_len,vxst_len]),high=len(data) - np.max([return_shift,vix_shift, vxst_shift]))\n",
    "            x[j, 0:return_len] = data.Return[i-return_len+return_shift:i+return_shift]\n",
    "            x[j, return_len:return_len+vix_len] = data.VIX[i-vix_len+vix_shift:i+vix_shift]\n",
    "            x[j, return_len+vix_len:return_len+vix_len+vxst_len] = data.VXST[i-vxst_len+vxst_shift:i+vxst_shift]\n",
    "            y[j] = data.VXST[i]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Iteration: 0\n",
      "Train MSE: 0.039190\n",
      "Validation MSE: 0.034901\n",
      "Iteration: 1000\n",
      "Train MSE: 0.076426\n",
      "Validation MSE: 0.025577\n",
      "Iteration: 2000\n",
      "Train MSE: 0.030727\n",
      "Validation MSE: 0.022503\n",
      "Iteration: 3000\n",
      "Train MSE: 0.022974\n",
      "Validation MSE: 0.015010\n",
      "Iteration: 4000\n",
      "Train MSE: 0.027306\n",
      "Validation MSE: 0.009895\n",
      "Iteration: 5000\n",
      "Train MSE: 0.010435\n",
      "Validation MSE: 0.010232\n",
      "Iteration: 6000\n",
      "Train MSE: 0.005542\n",
      "Validation MSE: 0.007674\n",
      "Iteration: 7000\n",
      "Train MSE: 0.004776\n",
      "Validation MSE: 0.007469\n",
      "Iteration: 8000\n",
      "Train MSE: 0.009482\n",
      "Validation MSE: 0.007514\n",
      "Iteration: 9000\n",
      "Train MSE: 0.008460\n",
      "Validation MSE: 0.007060\n",
      "Iteration: 10000\n",
      "Train MSE: 0.004876\n",
      "Validation MSE: 0.006304\n",
      "Iteration: 11000\n",
      "Train MSE: 0.004421\n",
      "Validation MSE: 0.004598\n",
      "Iteration: 12000\n",
      "Train MSE: 0.004979\n",
      "Validation MSE: 0.003710\n",
      "Iteration: 13000\n",
      "Train MSE: 0.005972\n",
      "Validation MSE: 0.002042\n",
      "Iteration: 14000\n",
      "Train MSE: 0.006193\n",
      "Validation MSE: 0.002984\n",
      "Iteration: 15000\n",
      "Train MSE: 0.005544\n",
      "Validation MSE: 0.004332\n",
      "Iteration: 16000\n",
      "Train MSE: 0.007066\n",
      "Validation MSE: 0.003223\n",
      "Iteration: 17000\n",
      "Train MSE: 0.002877\n",
      "Validation MSE: 0.003193\n",
      "Iteration: 18000\n",
      "Train MSE: 0.003915\n",
      "Validation MSE: 0.002022\n",
      "Iteration: 19000\n",
      "Train MSE: 0.003583\n",
      "Validation MSE: 0.001664\n",
      "Iteration: 20000\n",
      "Train MSE: 0.002182\n",
      "Validation MSE: 0.002244\n",
      "Iteration: 21000\n",
      "Train MSE: 0.003467\n",
      "Validation MSE: 0.001850\n",
      "Iteration: 22000\n",
      "Train MSE: 0.002848\n",
      "Validation MSE: 0.002427\n",
      "Iteration: 23000\n",
      "Train MSE: 0.001863\n",
      "Validation MSE: 0.001603\n",
      "Iteration: 24000\n",
      "Train MSE: 0.001711\n",
      "Validation MSE: 0.001906\n",
      "Iteration: 25000\n",
      "Train MSE: 0.002338\n",
      "Validation MSE: 0.001048\n",
      "Iteration: 26000\n",
      "Train MSE: 0.003371\n",
      "Validation MSE: 0.001303\n",
      "Iteration: 27000\n",
      "Train MSE: 0.001421\n",
      "Validation MSE: 0.001646\n"
     ]
    }
   ],
   "source": [
    "train_dataset = copy.deepcopy(dataset_norm[0:1200])\n",
    "train_dataset.reset_index(inplace=True)\n",
    "vali_dataset = copy.deepcopy(dataset_norm[1200:])\n",
    "vali_dataset.reset_index(inplace=True)\n",
    "\n",
    "train_gen = batch_generator(train_dataset, batch_size)\n",
    "vali_gen = batch_generator(vali_dataset, batch_size)\n",
    "\n",
    "print('Starting Training...')\n",
    "for i in range(steps+1):\n",
    "    x_batch, y_batch = next(train_gen)\n",
    "    sess.run(train_step, feed_dict={x: x_batch, y: y_batch})\n",
    "    if not i%refresh_rate:\n",
    "        print('Iteration: '+str(i))\n",
    "        x_vali, y_vali = next(vali_gen)\n",
    "        train_loss = sess.run(loss, feed_dict={x: x_batch, y: y_batch})\n",
    "        vali_loss = sess.run(loss, feed_dict={x: x_vali, y: y_vali})\n",
    "        print('Train MSE: {0:2.6f}'.format(train_loss))\n",
    "        print('Validation MSE: {0:2.6f}'.format(vali_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(low=np.max([return_len,vix_len,vxst_len]),high=len(dataset) - np.max([return_shift,vix_shift, vxst_shift]))\n",
    "\n",
    "x_vec = np.nan * np.ones((1, return_len+vix_len+vxst_len))\n",
    "x_vec[0, 0:return_len] = dataset_norm.Return[i-return_len+return_shift:i+return_shift]\n",
    "return_sel = dataset_norm[i-return_len+return_shift:i+return_shift]\n",
    "x_vec[0, return_len:return_len+vix_len] = dataset_norm.VIX[i-vix_len+vix_shift:i+vix_shift]\n",
    "vix_sel = dataset[i-vix_len+vix_shift:i+vix_shift]\n",
    "x_vec[0, return_len+vix_len:return_len+vix_len+vxst_len] = dataset_norm.VXST[i-vxst_len+vxst_shift:i+vxst_shift]\n",
    "vxst_sel = dataset[i-vxst_len+vxst_shift:i+vxst_shift]\n",
    "vxst_y = dataset.VXST[i]\n",
    "\n",
    "prediction = sess.run(output, feed_dict={x: x_vec}) * vxst_maxnorm * vxst_std + vxst_mean\n",
    "#prediction = (sess.run(output, feed_dict={x: x_vec})+ 1) * (vxst_max-vxst_min) / 2  + vxst_min\n",
    "\n",
    "print(dataset.Date[i])\n",
    "plt.figure()\n",
    "plt.plot(dataset.Date[i], vxst_y, marker='x', ls='')\n",
    "plt.plot(dataset.Date[i], prediction, marker='x', ls='')\n",
    "plt.plot(return_sel.Date, return_sel.Return, marker='x',ls='')\n",
    "plt.plot(vix_sel.Date,vix_sel.VIX, marker='x')\n",
    "plt.plot(vxst_sel.Date,vxst_sel.VXST, marker='x')\n",
    "plt.legend(['VXST', 'Prediction', 'Returns in', 'VIX in', 'VXST in'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VXST Simulation and Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error = np.nan * np.ones((len(dataset),1))\n",
    "prediction = np.nan * np.ones((len(dataset),1))\n",
    "prediction_norm = np.nan * np.ones((len(dataset),1))\n",
    "prediction_date = list()\n",
    "for i in range(np.max([return_len,vix_len,vxst_len]),len(dataset) - np.max([return_shift,vix_shift, vxst_shift])):\n",
    "    x_vec = np.nan * np.ones((1, return_len+vix_len+vxst_len))\n",
    "    x_vec[0, 0:return_len] = dataset_norm.Return[i-return_len+return_shift:i+return_shift]\n",
    "    x_vec[0, return_len:return_len+vix_len] = dataset_norm.VIX[i-vix_len+vix_shift:i+vix_shift]\n",
    "    x_vec[0, return_len+vix_len:return_len+vix_len+vxst_len] = dataset_norm.VXST[i-vxst_len+vxst_shift:i+vxst_shift]\n",
    "    vxst_y = dataset.VXST[i]\n",
    "    prediction_norm[i,0] = sess.run(output, feed_dict={x: x_vec})\n",
    "    prediction[i,0] = prediction_norm[i,0] * vxst_maxnorm * vxst_std + vxst_mean\n",
    "    #prediction[i,0] = (prediction_norm[i,0] + 1) * (vxst_max-vxst_min) / 2 + vxst_min\n",
    "    prediction_date.append(dataset.Date[i])\n",
    "    error[i,0] = vxst_y - prediction[i,0]  \n",
    "error = error[np.where(~np.isnan(error))]\n",
    "prediction = prediction[np.where(~np.isnan(prediction))]\n",
    "prediction_norm = prediction_norm[np.where(~np.isnan(prediction_norm))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dataset_norm.Date, dataset.VXST)\n",
    "plt.plot(prediction_date, prediction)\n",
    "plt.title('VXST Simulation')\n",
    "plt.legend(['VXST','Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(error,50);\n",
    "plt.title('Error (%pts)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[-1,1] NORM\n",
    "dataset_norm = copy.deepcopy(dataset)\n",
    "\n",
    "vix_min = np.min(dataset_norm.VIX)\n",
    "vix_max = np.max(dataset_norm.VIX)\n",
    "dataset_norm.VIX = -1 + (dataset_norm.VIX - vix_min) * 2 / (vix_max-vix_min)\n",
    "\n",
    "vxst_min = np.min(dataset_norm.VXST)\n",
    "vxst_max = np.max(dataset_norm.VXST)\n",
    "dataset_norm.VXST = -1 + (dataset_norm.VXST - vxst_min) * 2 / (vxst_max-vxst_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
